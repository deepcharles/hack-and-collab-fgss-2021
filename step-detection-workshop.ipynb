{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IA and locomotion: human gait analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Context**\n",
    "\n",
    "The study of human gait is a central problem in medical research with far-reaching consequences in the public health domain.\n",
    "This complex mechanism can be altered by a wide range of pathologies (such as Parkinson’s disease, arthritis, stroke,...), often resulting in a significant loss of autonomy and an increased risk of fall.\n",
    "Understanding the influence of such medical disorders on a subject's gait would greatly facilitate early detection and prevention of those possibly harmful situations.\n",
    "To address these issues, clinical and bio-mechanical researchers have worked to objectively quantify gait characteristics.\n",
    "\n",
    "\n",
    "Among the gait features that have proved their relevance in a medical context, several are linked to the notion of step (step duration, variation in step length, etc.), which can be seen as the core atom of the locomotion process.\n",
    "Many algorithms have therefore been developed to automatically (or semi-automatically) detect gait events (such as heel-strikes, heel-off, etc.) from accelerometer/gyrometer signals.\n",
    "\n",
    "Most of the time, the algorithms used for step detection are dedicated to a specific population (healthy subjects, elderly subjects, Parkinson patients, etc.) and only a few publications deal with heterogeneous populations composed of several types of subjects.\n",
    "Another limit to existing algorithms is that they often focus on locomotion in established regime (once the subject has initiated its gait) and do not deal with steps during U-turn, gait initiation or gait termination.\n",
    "Yet, initiation and termination steps are particularly sensitive to pathological states.\n",
    "For example, the first step of Parkinsonian patients has been described as slower and smaller that the first step of age-matched subjects.\n",
    "U-turn steps are also interesting since 45% of daily living walking is made up of turning steps, and when compared to straight-line walking, turning has been emphasized as a high-risk fall situation.\n",
    "This argues for reliable algorithms that could detect initiation, termination and turning steps in both healthy and pathological subjects.\n",
    "\n",
    "\n",
    "**Step detection**\n",
    "\n",
    "The objective is to recognize the **start and end times of footsteps** contained in accelerometer and gyrometer signals recorded with Inertial Measurement Units (IMUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from loadmydata.load_human_locomotion import (\n",
    "    get_code_list,\n",
    "    load_human_locomotion_dataset,\n",
    ")\n",
    "\n",
    "from convsparsecoder import ConvSparseCoder\n",
    "from locogram import get_locogram\n",
    "from utils import pad_at_the_end, plot_CDL, plot_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal(sensor_data, dim_name=\"LRY\") -> np.ndarray:\n",
    "    \"\"\"Select a signal from a given trial.\"\"\"\n",
    "    # choose a single dimension\n",
    "    signal = sensor_data.signal[dim_name].to_numpy()\n",
    "    signal -= signal.mean()\n",
    "    signal /= signal.std()\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 3)):\n",
    "    return plt.subplots(figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and clinical protocol\n",
    "\n",
    "#### Participants\n",
    "\n",
    "The data was collected between April 2014 and October 2015 by monitoring healthy (control) subjects and patients from several medical departments (see [publication](#Publication) for more information).\n",
    "Participants are divided into three groups depending on their impairment:\n",
    "- **Healthy** subjects had no known medical impairment.\n",
    "- The **orthopedic group** is composed of 2 cohorts of distinct pathologies: lower limb osteoarthrosis and cruciate ligament injury.\n",
    "- The **neurological group** is composed of 4 cohorts: hemispheric stroke, Parkinson's disease, toxic peripheral neuropathy and radiation induced leukoencephalopathy.\n",
    "\n",
    "Note that certain participants were recorded on multiple occasions, therefore several trials may correspond to the same person.\n",
    "In the training set and in the testing set, the proportion of trials coming from the \"healthy\", \"orthopedic\" and \"neurological\" groups is roughly the same, 24%, 24% and 52% respectively.\n",
    "\n",
    "#### Protocol and equipment\n",
    "\n",
    "All subjects underwent the same protocol described below. First, a IMU (Inertial Measurement Unit) that recorded accelerations and angular velocities was attached to each foot.\n",
    "All signals have been acquired at 100 Hz with two brands of IMUs: XSens&trade; and Technoconcept&reg;.\n",
    "One brand of IMU was attached to the dorsal face of each foot.\n",
    "(Both feet wore the same brand.)\n",
    "After sensor fixation, participants were asked to perform the following sequence of activities:\n",
    "- stand for 6 s,\n",
    "- walk 10 m at preferred walking speed on a level surface to a previously shown turn point,\n",
    "- turn around (without previous specification of a turning side),\n",
    "- walk back to the starting point,\n",
    "- stand for 2 s.\n",
    "\n",
    "Subjects walked at their comfortable speed with their shoes and without walking aid.\n",
    "This protocol is schematically illustrated in the following figure.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/protocol-schema.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Each IMU records its acceleration and angular velocity in the $(X, Y, Z, V)$ set of axes defined in the following figure.\n",
    "The $V$ axis is aligned with gravity, while the $X$, $Y$ and $Z$ axes are attached to the sensor.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/sensor-photo.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/sensor-position.png\" width=\"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step detection in a clinical context\n",
    "\n",
    "The following schema describes how step detection methods are integrated in a clinical context.\n",
    "<br/><br/>\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/step-detection-schema.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "(1) During a trial, sensors send their own acceleration and angular velocity to the physician's computer.\n",
    "\n",
    "(2) A software on the physician's computer synchronizes the data sent from both sensors and produces two multivariate signals (of same shape), each corresponding to a foot.\n",
    "\n",
    "\n",
    "A step detection procedure is applied on each signal to produce two lists of footsteps (one per foot/sensor).\n",
    "The numbers of left footsteps and right footsteps are not necessarily the same.\n",
    "Indeed, subjects often have a preferred foot to initiate and terminate a walk or a u-turn, resulting in one or more footsteps from this preferred foot.\n",
    "The starts and ends of footsteps are then used to create meaningful features to characterize the subject's gait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "During a trial, a subject executes the protocol described above.\n",
    "This produces two multivariates signals (one for each foot/sensor) and for each signal, a number of footsteps have be annotated.\n",
    "In addition, information (metadata) about the trial and participant are provided.\n",
    "All three elements (signal, step annotation and metadata) are detailled in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wil download the data on the first run\n",
    "trial_1 = load_human_locomotion_dataset(\"17-2\")\n",
    "trial_2 = load_human_locomotion_dataset(\"56-2\")\n",
    "\n",
    "print(trial_1.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal\n",
    "\n",
    "Each IMU that the participants wore provided $\\mathbb{R}^{8}$-valued signals, sampled at 100 Hz.\n",
    "In this setting, each dimension is defined by the foot (`L` for left, `R` for right), the signal type (`A` for acceleration, `R` for angular velocity) and the axis (`X`, `Y`, `Z` or `V`).\n",
    "For instance, `RRX` denotes the angular velocity around the `X`-axis of the right foot.\n",
    "Accelerations are given in $m/s^2$ and angular velocities, in $deg/s$.\n",
    "The signal is available in the `.signal` attribute as a `Pandas` dataframe.\n",
    "\n",
    "Note that this multivariate signal originates from a two sensors (one on each foot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signal is available in the `signal` attribute.\n",
    "\n",
    "fig, (ax_0, ax_1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 3))\n",
    "\n",
    "# Here we show the left foot (`L`)\n",
    "trial_1.signal[[\"LAX\", \"LAY\", \"LAZ\", \"LAV\"]].plot(\n",
    "    ax=ax_0\n",
    ")  # select the accelerations\n",
    "trial_1.signal[[\"LRX\", \"LRY\", \"LRZ\", \"LRV\"]].plot(\n",
    "    ax=ax_1\n",
    ")  # select the angular velocities\n",
    "\n",
    "trial_1.signal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"flat part\" at the beginning of each dimension is the result of the participants standing still for a few\n",
    "seconds before walking (see [Protocol](#Protocol-and-equipment)).\n",
    "The same behaviour can be seen at the end of each dimension (often but not always), though for a quite smaller duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Metadata\n",
    "A number of metadata (either numerical or categorical) are provided for each sensor recording, detailing the participant being monitored and the sensor position:\n",
    "\n",
    "- `trial_code`: unique identifier for the trial;\n",
    "- `age` (in years);\n",
    "- `gender`: male (\"M\") or female (\"F\");\n",
    "- `height` (in meters);\n",
    "- `weight` (in kilograms);\n",
    "- `bmi` (in kg/m2): body mass index;\n",
    "- `laterality`: subject's \"footedness\" or \"foot to kick a ball\" (\"Left\", \"Right\" or \"Ambidextrous\").\n",
    "- `sensor`: brand of the IMU used for the recording (“XSens” or “TCon”);\n",
    "- `pathology_group`: this variable takes value in {“Healthy”, “Orthopedic”, “Neurological”};\n",
    "- `is_control`: whether the subject is a control subject (\"Yes\" or \"No\");\n",
    "- `foot`: foot on which the sensor was attached (\"Left\" or \"Right\").\n",
    "\n",
    "These are accessible using the notation `sensor_data.metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 10 + \" Trial 1 \" + \"-\" * 10)\n",
    "print(trial_1.metadata)\n",
    "print(\"-\" * 10 + \" Trial 2 \" + \"-\" * 10)\n",
    "print(trial_2.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step annotation (the \"label\" to predict)\n",
    "Footsteps were manually annotated by specialists using a software that displayed the signals from the relevant sensor (left or right foot) and allowed the specialist to indicate the starts and ends of each step.\n",
    "\n",
    "A footstep is defined as the period during which the foot is moving.\n",
    "Footsteps are separated by periods when the foot is still and flat on the floor.\n",
    "Therefore, in our setting, a footstep starts with a heel-off and ends with the following toe-strike of the same foot.\n",
    "\n",
    "\n",
    "Footsteps (the \"label\" to predict from the signal) are contained in a list whose elements are list of two integers, the start and end indexes. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left foot\n",
    "print(trial_1.left_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = f\"{trial_1.left_steps.shape[0]} footsteps were annotated on the left foot, and {trial_1.right_steps.shape[0]} on the right.\"\n",
    "print(msg)\n",
    "\n",
    "# plot steps\n",
    "plot_steps(sensor_data=trial_1, left_or_right=\"right\", choose_step=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of footsteps and signals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On the first two plots.**\n",
    "The repeated patterns (colored in light green) correspond to periods when the foot is moving.\n",
    "During the non-annotated periods, the foot is flat and not moving and the signals are constant.\n",
    "Generally, steps at the beginning and end of the recording, as well as during the u-turn (in the middle of the signal approximatively, see [Protocol](#Protocol-and-equipment)) are a bit different from the other ones.\n",
    "\n",
    "**On the last two plots.** A close-up on a single footstep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax_left, ax_right) = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "locogram = get_locogram(sensor_data=trial_2, left_or_right=\"left\")\n",
    "_ = sns.heatmap(1 - locogram, ax=ax_left)\n",
    "ax_left.set_title(\"Left foot\")\n",
    "\n",
    "locogram = get_locogram(sensor_data=trial_2, left_or_right=\"right\")\n",
    "_ = sns.heatmap(1 - locogram, ax=ax_right)\n",
    "_ = ax_right.set_title(\"Right foot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax_left, ax_right) = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "locogram = get_locogram(sensor_data=trial_1, left_or_right=\"left\")\n",
    "_ = sns.heatmap(1 - locogram, ax=ax_left)\n",
    "ax_left.set_title(\"Left foot\")\n",
    "\n",
    "locogram = get_locogram(sensor_data=trial_1, left_or_right=\"right\")\n",
    "_ = sns.heatmap(1 - locogram, ax=ax_right)\n",
    "_ = ax_right.set_title(\"Right foot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Compare the left and right locograms.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# synthetic atoms\n",
    "atom_1 = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "atom_2 = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.33066907e-16, 6.06060606e-02, 1.21212121e-01, 1.81818182e-01, 2.42424242e-01, 3.03030303e-01, 3.63636364e-01, 4.24242424e-01, 4.84848485e-01, 5.45454545e-01, 6.06060606e-01, 6.66666667e-01, 7.27272727e-01, 7.87878788e-01, 8.48484848e-01, 9.09090909e-01, 9.69696970e-01, 1.03030303e00, 1.09090909e00, 1.15151515e00, 1.21212121e00, 1.27272727e00, 1.33333333e00, 1.39393939e00, 1.45454545e00, 1.51515152e00, 1.57575758e00, 1.63636364e00, 1.69696970e00, 1.75757576e00, 1.81818182e00, 1.87878788e00, 1.93939394e00, 5.55111512e-16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "# fmt: on\n",
    "\n",
    "atom_width = atom_1.shape[0]\n",
    "\n",
    "fig, (ax_left, ax_right) = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "ax_left.plot(atom_1, \"k\", lw=1)\n",
    "ax_left.set_title(\"Atom 1\")\n",
    "ax_right.plot(atom_2, \"k\", lw=1)\n",
    "_ = ax_right.set_title(\"Atom 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "# random activitions\n",
    "activations_1 = np.random.binomial(n=1, p=0.005, size=n_samples)\n",
    "activations_2 = np.random.binomial(n=1, p=0.001, size=n_samples)\n",
    "\n",
    "fig, ax = fig_ax()\n",
    "_ = ax.plot(activations_1, label=\"activations 1\")\n",
    "_ = ax.plot(activations_2, label=\"activations 2\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "signal = np.convolve(activations_1, atom_1, mode=\"same\")\n",
    "signal += np.convolve(activations_2, atom_2, mode=\"same\")\n",
    "\n",
    "fig, ax = fig_ax()\n",
    "_ = ax.plot(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse coding\n",
    "\n",
    "The optimization problem to find the activations is:\n",
    "$$\n",
    "        \\mathbf{Z}^\\star = \\arg\\min_{(\\mathbf{z}_k)} \\left\\| \\mathbf{x} - \\sum_{k=1}^K (\\mathbf{z}_k \\star \\mathbf{d}_k )\\right\\|_2^2 \\quad + \\quad \\lambda \\sum_{k=1}^K \\|\\mathbf{z}_k \\|_1\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathbf{x}=[x_1, x_2, \\dots, x_T]$ is a univariate signal with $T$ samples;\n",
    "- $\\mathbf{d}_k$ ($k=1,\\dots,K$) are $K$ patterns of length $L$;\n",
    "- $\\mathbf{z}_k$ of length $N-L+1$ is the activation signal of pattern $\\mathbf{d}_k$;\n",
    "- $\\lambda>0$ controls the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate atoms\n",
    "atoms = np.c_[atom_1, atom_2].T\n",
    "# sparse coding\n",
    "coder = ConvSparseCoder(atoms=atoms, positive_code=True).fit(\n",
    "    signal=signal, penalty=10\n",
    ")\n",
    "sparse_codes = coder.sparse_codes\n",
    "reconstruction = coder.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax()\n",
    "\n",
    "ax.plot(signal, label=\"Original\")\n",
    "ax.plot(reconstruction, label=\"Reconstruction\")\n",
    "ax.set_title(f\"MSE: {((signal-reconstruction)**2).mean():.3f}\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDL(\n",
    "    signal,\n",
    "    codes=sparse_codes,\n",
    "    atoms=atoms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>What do you observe when the penalty increases?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On real-world data\n",
    "\n",
    "We apply the same methodology on the Gait data set.\n",
    "\n",
    "We only consider two trials with different walking patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_1 = get_signal(trial_1, dim_name=\"LRY\")\n",
    "signal_2 = get_signal(trial_2, dim_name=\"LRY\")\n",
    "\n",
    "# take an arbitrary footstep\n",
    "start, end = trial_1.left_steps[4]\n",
    "template_1 = signal_1[start:end]\n",
    "template_1 -= template_1.mean()\n",
    "template_1 /= template_1.std()\n",
    "\n",
    "start, end = trial_2.left_steps[7]\n",
    "template_2 = signal_2[start:end]\n",
    "template_2 -= template_2.mean()\n",
    "template_2 /= template_2.std()\n",
    "\n",
    "# Pad the atoms to the same length\n",
    "template_length = max(template_1.shape[0], template_2.shape[0])\n",
    "template_1 = pad_at_the_end(template_1, desired_length=template_length)\n",
    "template_2 = pad_at_the_end(template_2, desired_length=template_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the convolutional sparse coder\n",
    "coder = ConvSparseCoder(\n",
    "    atoms=np.c_[template_1, template_2].T, positive_code=True\n",
    ")\n",
    "\n",
    "plt.plot(coder.atoms[0], label=\"Atom 1\")\n",
    "plt.plot(coder.atoms[1], label=\"Atom 2\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse coding\n",
    "reconstruction_1 = coder.fit(signal=signal_1, penalty=20).predict()\n",
    "# Plot\n",
    "plot_CDL(\n",
    "    signal_1,\n",
    "    codes=coder.sparse_codes,\n",
    "    atoms=coder.atoms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sparse coding\n",
    "reconstruction_2 = coder.fit(signal=signal_2, penalty=80).predict()\n",
    "# Plot\n",
    "plot_CDL(\n",
    "    signal_2,\n",
    "    codes=coder.sparse_codes,\n",
    "    atoms=coder.atoms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Comment on the activations patterns.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
