{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IA and locomotion: human gait analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Context**\n",
    "\n",
    "The study of human gait is a central problem in medical research with far-reaching consequences in the public health domain.\n",
    "This complex mechanism can be altered by a wide range of pathologies (such as Parkinson’s disease, arthritis, stroke,...), often resulting in a significant loss of autonomy and an increased risk of fall.\n",
    "Understanding the influence of such medical disorders on a subject's gait would greatly facilitate early detection and prevention of those possibly harmful situations.\n",
    "To address these issues, clinical and bio-mechanical researchers have worked to objectively quantify gait characteristics.\n",
    "\n",
    "\n",
    "Among the gait features that have proved their relevance in a medical context, several are linked to the notion of step (step duration, variation in step length, etc.), which can be seen as the core atom of the locomotion process.\n",
    "Many algorithms have therefore been developed to automatically (or semi-automatically) detect gait events (such as heel-strikes, heel-off, etc.) from accelerometer/gyrometer signals.\n",
    "\n",
    "Most of the time, the algorithms used for step detection are dedicated to a specific population (healthy subjects, elderly subjects, Parkinson patients, etc.) and only a few publications deal with heterogeneous populations composed of several types of subjects.\n",
    "Another limit to existing algorithms is that they often focus on locomotion in established regime (once the subject has initiated its gait) and do not deal with steps during U-turn, gait initiation or gait termination.\n",
    "Yet, initiation and termination steps are particularly sensitive to pathological states.\n",
    "For example, the first step of Parkinsonian patients has been described as slower and smaller that the first step of age-matched subjects.\n",
    "U-turn steps are also interesting since 45% of daily living walking is made up of turning steps, and when compared to straight-line walking, turning has been emphasized as a high-risk fall situation.\n",
    "This argues for reliable algorithms that could detect initiation, termination and turning steps in both healthy and pathological subjects.\n",
    "\n",
    "\n",
    "**Step detection**\n",
    "\n",
    "The objective is to recognize the **start and end times of footsteps** contained in accelerometer and gyrometer signals recorded with Inertial Measurement Units (IMUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from loadmydata.load_human_locomotion import (get_code_list,\n",
    "                                              load_human_locomotion_dataset)\n",
    "from metric import fscore\n",
    "from scipy.linalg import circulant\n",
    "from scipy.signal import argrelmax\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import SparseCoder\n",
    "from sklearn.utils import Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_min_max(a_list) -> str:\n",
    "    \"\"\"[a_1, a_2,...] -> 'avg (min: minimum, max: maximum)'\"\"\"\n",
    "    return f\"{np.mean(a_list):.1f} (min: {np.min(a_list):.1f}, max: {np.max(a_list):.1f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(signal: np.ndarray, desired_length: int) -> np.ndarray:\n",
    "    \"\"\"Add zeros at the start and end of a signal until it reached the desired lenght.\n",
    "\n",
    "    The same number of zeros is added on each side, except when desired_length-signal.shape[0] is odd,\n",
    "    in which case, there is one more zero at the beginning.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 1:\n",
    "        (n_samples,) = signal.shape\n",
    "        n_dims = 1\n",
    "    else:\n",
    "        n_samples, n_dims = signal.shape\n",
    "\n",
    "    assert desired_length >= n_samples\n",
    "\n",
    "    length_diff = desired_length - n_samples\n",
    "    pad_width_at_the_start = pad_width_at_the_end = length_diff // 2\n",
    "    pad_width_at_the_start += (\n",
    "        length_diff - pad_width_at_the_end - pad_width_at_the_start\n",
    "    )\n",
    "\n",
    "    return np.pad(\n",
    "        signal.reshape(n_samples, n_dims).astype(float),\n",
    "        pad_width=((pad_width_at_the_start, pad_width_at_the_end), (0, 0)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=(0,),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_at_the_end(signal: np.ndarray, desired_length: int) -> np.ndarray:\n",
    "    \"\"\"Add zeros at the end of a signal until it reached the desired length.\"\"\"\n",
    "    if signal.ndim == 1:\n",
    "        (n_samples,) = signal.shape\n",
    "        n_dims = 1\n",
    "    else:\n",
    "        n_samples, n_dims = signal.shape\n",
    "\n",
    "    assert desired_length >= n_samples\n",
    "\n",
    "    pad_width_at_the_end = desired_length - n_samples\n",
    "\n",
    "    return np.pad(\n",
    "        signal.reshape(n_samples, n_dims).astype(float),\n",
    "        pad_width=((0, pad_width_at_the_end), (0, 0)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=(0,),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 3)):\n",
    "    return plt.subplots(figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_codes(\n",
    "    signal: np.ndarray, dictionary: np.ndarray, penalty: float\n",
    "):\n",
    "    coder = SparseCoder(\n",
    "        dictionary=dictionary,\n",
    "        transform_algorithm=\"lasso_lars\",\n",
    "        transform_alpha=penalty,\n",
    "        positive_code=True,\n",
    "    )\n",
    "    return coder.transform(signal.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary_from_single_atom(atom, n_samples):\n",
    "    atom_width = atom.shape[0]\n",
    "    dictionary = circulant(pad_at_the_end(atom, n_samples).flatten())[\n",
    "        :, : n_samples - atom_width + 1\n",
    "    ].T\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CDL(signal, codes, atoms, figsize=(15, 10)):\n",
    "    \"\"\"Plot the learned dictionary `D` and the associated sparse codes `Z`.\n",
    "\n",
    "    `signal` is an univariate signal of shape (n_samples,) or (n_samples, 1).\n",
    "    \"\"\"\n",
    "    (n_atoms, atom_length) = atoms.shape\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(n_atoms + 1, 3, (2, 3))\n",
    "    plt.plot(signal)\n",
    "    for i in range(n_atoms):\n",
    "        plt.subplot(n_atoms + 1, 3, 3 * i + 4)\n",
    "        plt.plot(atoms[i])\n",
    "        plt.subplot(n_atoms + 1, 3, (3 * i + 5, 3 * i + 6))\n",
    "        plt.plot(codes[i])\n",
    "        plt.ylim((np.min(codes), np.max(codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_size(signal, desired_size=100):\n",
    "    return np.interp(\n",
    "        np.arange(desired_size),\n",
    "        np.linspace(0, desired_size, signal.shape[0]),\n",
    "        signal,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locogram(sensor_data, left_or_right=\"left\"):\n",
    "    if left_or_right == \"left\":\n",
    "        steps = sensor_data.left_steps\n",
    "        acc_columns = [\"LAX\", \"LAY\", \"LAZ\"]\n",
    "    elif left_or_right == \"right\":\n",
    "        steps = sensor_data.right_steps\n",
    "        acc_columns = [\"RAX\", \"RAY\", \"RAZ\"]\n",
    "\n",
    "    n_steps = steps.shape[0]\n",
    "    locogram = np.zeros((n_steps, n_steps))\n",
    "\n",
    "    acc_norm = np.linalg.norm(\n",
    "        sensor_data.signal[acc_columns].to_numpy(), axis=1\n",
    "    )\n",
    "\n",
    "    for step_ind_1 in range(n_steps):\n",
    "        start, end = steps[step_ind_1]\n",
    "        step_1 = resample_to_size(acc_norm[start:end])\n",
    "        for step_ind_2 in range(step_ind_1 + 1, n_steps):\n",
    "            start, end = steps[step_ind_2]\n",
    "            step_2 = resample_to_size(acc_norm[start:end])\n",
    "            locogram[step_ind_1, step_ind_2] = pearsonr(step_1, step_2)[0]\n",
    "\n",
    "    locogram += locogram.T\n",
    "    np.fill_diagonal(a=locogram, val=1.0)\n",
    "\n",
    "    return locogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wil download the data on the first run\n",
    "_ = load_human_locomotion_dataset(\"1-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and clinical protocol\n",
    "\n",
    "#### Participants\n",
    "\n",
    "The data was collected between April 2014 and October 2015 by monitoring healthy (control) subjects and patients from several medical departments (see [publication](#Publication) for more information).\n",
    "Participants are divided into three groups depending on their impairment:\n",
    "- **Healthy** subjects had no known medical impairment.\n",
    "- The **orthopedic group** is composed of 2 cohorts of distinct pathologies: lower limb osteoarthrosis and cruciate ligament injury.\n",
    "- The **neurological group** is composed of 4 cohorts: hemispheric stroke, Parkinson's disease, toxic peripheral neuropathy and radiation induced leukoencephalopathy.\n",
    "\n",
    "Note that certain participants were recorded on multiple occasions, therefore several trials may correspond to the same person.\n",
    "In the training set and in the testing set, the proportion of trials coming from the \"healthy\", \"orthopedic\" and \"neurological\" groups is roughly the same, 24%, 24% and 52% respectively.\n",
    "\n",
    "#### Protocol and equipment\n",
    "\n",
    "All subjects underwent the same protocol described below. First, a IMU (Inertial Measurement Unit) that recorded accelerations and angular velocities was attached to each foot.\n",
    "All signals have been acquired at 100 Hz with two brands of IMUs: XSens&trade; and Technoconcept&reg;.\n",
    "One brand of IMU was attached to the dorsal face of each foot.\n",
    "(Both feet wore the same brand.)\n",
    "After sensor fixation, participants were asked to perform the following sequence of activities:\n",
    "- stand for 6 s,\n",
    "- walk 10 m at preferred walking speed on a level surface to a previously shown turn point,\n",
    "- turn around (without previous specification of a turning side),\n",
    "- walk back to the starting point,\n",
    "- stand for 2 s.\n",
    "\n",
    "Subjects walked at their comfortable speed with their shoes and without walking aid.\n",
    "This protocol is schematically illustrated in the following figure.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/protocol-schema.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Each IMU records its acceleration and angular velocity in the $(X, Y, Z, V)$ set of axes defined in the following figure.\n",
    "The $V$ axis is aligned with gravity, while the $X$, $Y$ and $Z$ axes are attached to the sensor.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/sensor-photo.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/sensor-position.png\" width=\"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step detection in a clinical context\n",
    "\n",
    "The following schema describes how step detection methods are integrated in a clinical context.\n",
    "<br/><br/>\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/step-detection-schema.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "(1) During a trial, sensors send their own acceleration and angular velocity to the physician's computer.\n",
    "\n",
    "(2) A software on the physician's computer synchronizes the data sent from both sensors and produces two multivariate signals (of same shape), each corresponding to a foot.\n",
    "\n",
    "\n",
    "A step detection procedure is applied on each signal to produce two lists of footsteps (one per foot/sensor).\n",
    "The numbers of left footsteps and right footsteps are not necessarily the same.\n",
    "Indeed, subjects often have a preferred foot to initiate and terminate a walk or a u-turn, resulting in one or more footsteps from this preferred foot.\n",
    "The starts and ends of footsteps are then used to create meaningful features to characterize the subject's gait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "During a trial, a subject executes the protocol described above.\n",
    "This produces two multivariates signals (one for each foot/sensor) and for each signal, a number of footsteps have be annotated.\n",
    "In addition, information (metadata) about the trial and participant are provided.\n",
    "All three elements (signal, step annotation and metadata) are detailled in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosing a trial\n",
    "code_list = get_code_list()\n",
    "code = code_list[90]\n",
    "# loading a trial\n",
    "sensor_data = load_human_locomotion_dataset(code)\n",
    "# print data set description\n",
    "print(sensor_data.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment any of the following line the show the attributes of `data`.\n",
    "\n",
    "print(sensor_data.signal)  # pandas array\n",
    "# print(sensor_data.left_steps)  # numpy array (n_left_steps, 2)\n",
    "# print(sensor_data.right_steps)  # numpy array (n_right_steps, 2)\n",
    "print(sensor_data.metadata)  # dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal\n",
    "\n",
    "Each IMU that the participants wore provided $\\mathbb{R}^{8}$-valued signals, sampled at 100 Hz.\n",
    "In this setting, each dimension is defined by the foot (`L` for left, `R` for right), the signal type (`A` for acceleration, `R` for angular velocity) and the axis (`X`, `Y`, `Z` or `V`).\n",
    "For instance, `RRX` denotes the angular velocity around the `X`-axis of the right foot.\n",
    "Accelerations are given in $m/s^2$ and angular velocities, in $deg/s$.\n",
    "The signal is available in the `.signal` attribute as a `Pandas` dataframe.\n",
    "\n",
    "Note that this multivariate signal originates from a two sensors (one on each foot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signal is available in the `signal` attribute.\n",
    "\n",
    "fig, (ax_0, ax_1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 3))\n",
    "\n",
    "# Here we show the left foot (`L`)\n",
    "sensor_data.signal[[\"LAX\", \"LAY\", \"LAZ\", \"LAV\"]].plot(\n",
    "    ax=ax_0\n",
    ")  # select the accelerations\n",
    "sensor_data.signal[[\"LRX\", \"LRY\", \"LRZ\", \"LRV\"]].plot(\n",
    "    ax=ax_1\n",
    ")  # select the angular velocities\n",
    "\n",
    "sensor_data.signal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"flat part\" at the beginning of each dimension is the result of the participants standing still for a few\n",
    "seconds before walking (see [Protocol](#Protocol-and-equipment)).\n",
    "The same behaviour can be seen at the end of each dimension (often but not always), though for a quite smaller duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Metadata\n",
    "A number of metadata (either numerical or categorical) are provided for each sensor recording, detailing the participant being monitored and the sensor position:\n",
    "\n",
    "- `trial_code`: unique identifier for the trial;\n",
    "- `age` (in years);\n",
    "- `gender`: male (\"M\") or female (\"F\");\n",
    "- `height` (in meters);\n",
    "- `weight` (in kilograms);\n",
    "- `bmi` (in kg/m2): body mass index;\n",
    "- `laterality`: subject's \"footedness\" or \"foot to kick a ball\" (\"Left\", \"Right\" or \"Ambidextrous\").\n",
    "- `sensor`: brand of the IMU used for the recording (“XSens” or “TCon”);\n",
    "- `pathology_group`: this variable takes value in {“Healthy”, “Orthopedic”, “Neurological”};\n",
    "- `is_control`: whether the subject is a control subject (\"Yes\" or \"No\");\n",
    "- `foot`: foot on which the sensor was attached (\"Left\" or \"Right\").\n",
    "\n",
    "These are accessible using the notation `sensor_data.metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_metadata = Bunch(**sensor_data.metadata)\n",
    "\n",
    "msg = f\"\"\"Metadata:\n",
    "age\\t\\t{trial_metadata.Age},\n",
    "gender\\t\\t{trial_metadata.Gender},\n",
    "height\\t\\t{trial_metadata.Height},\n",
    "weight\\t\\t{trial_metadata.Height},\n",
    "bmi\\t\\t{trial_metadata.BMI},\n",
    "laterality\\t{trial_metadata.Laterality},\n",
    "sensor\\t\\t{trial_metadata.Sensor},\n",
    "pathology_group\\t{trial_metadata.PathologyGroup},\n",
    "trial code\\t{trial_metadata.Code}\"\"\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step annotation (the \"label\" to predict)\n",
    "Footsteps were manually annotated by specialists using a software that displayed the signals from the relevant sensor (left or right foot) and allowed the specialist to indicate the starts and ends of each step.\n",
    "\n",
    "A footstep is defined as the period during which the foot is moving.\n",
    "Footsteps are separated by periods when the foot is still and flat on the floor.\n",
    "Therefore, in our setting, a footstep starts with a heel-off and ends with the following toe-strike of the same foot.\n",
    "\n",
    "\n",
    "Footsteps (the \"label\" to predict from the signal) are contained in a list whose elements are list of two integers, the start and end indexes. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left foot\n",
    "print(sensor_data.left_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of footsteps and signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = f\"For the trial '{trial_metadata.Code}', {sensor_data.left_steps.shape[0]} footsteps were annotated on the left foot, and {sensor_data.right_steps.shape[0]} on the right.\"\n",
    "print(msg)\n",
    "\n",
    "\n",
    "# Color the footsteps\n",
    "fig, (ax_0, ax_1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 3))\n",
    "\n",
    "ax = ax_0\n",
    "sensor_data.signal[[\"LAX\", \"LAY\", \"LAZ\", \"LAV\"]].plot(ax=ax)\n",
    "line_args = {\"linestyle\": \"--\", \"color\": \"k\"}\n",
    "for (start, end) in sensor_data.left_steps:\n",
    "    ax.axvline(start, **line_args)\n",
    "    ax.axvline(end, **line_args)\n",
    "    ax.axvspan(start, end, facecolor=\"g\", alpha=0.3)\n",
    "\n",
    "ax = ax_1\n",
    "sensor_data.signal[[\"LRX\", \"LRY\", \"LRZ\", \"LRV\"]].plot(ax=ax)\n",
    "for (start, end) in sensor_data.left_steps:\n",
    "    ax.axvline(start, **line_args)\n",
    "    ax.axvline(end, **line_args)\n",
    "    ax.axvspan(start, end, facecolor=\"g\", alpha=0.3)\n",
    "\n",
    "\n",
    "# Close-up on a footstep\n",
    "fig, (ax_0, ax_1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 3))\n",
    "\n",
    "start, end = sensor_data.left_steps[4]\n",
    "\n",
    "ax = ax_0\n",
    "sensor_data.signal[[\"LAX\", \"LAY\", \"LAZ\", \"LAV\"]][start - 30 : end + 30].plot(\n",
    "    ax=ax\n",
    ")\n",
    "ax.axvline(start, **line_args)\n",
    "ax.axvline(end, **line_args)\n",
    "ax.axvspan(start, end, facecolor=\"g\", alpha=0.3)\n",
    "\n",
    "ax = ax_1\n",
    "sensor_data.signal[[\"LRX\", \"LRY\", \"LRZ\", \"LRV\"]][start - 30 : end + 30].plot(\n",
    "    ax=ax\n",
    ")\n",
    "ax.axvline(start, **line_args)\n",
    "ax.axvline(end, **line_args)\n",
    "_ = ax.axvspan(start, end, facecolor=\"g\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On the first two plots.**\n",
    "The repeated patterns (colored in light green) correspond to periods when the foot is moving.\n",
    "During the non-annotated periods, the foot is flat and not moving and the signals are constant.\n",
    "Generally, steps at the beginning and end of the recording, as well as during the u-turn (in the middle of the signal approximatively, see [Protocol](#Protocol-and-equipment)) are a bit different from the other ones.\n",
    "\n",
    "**On the last two plots.** A close-up on a single footstep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comments\n",
    "\n",
    "- Some metadata (namely `Age`, `Height`, `Weight`, `BMI` and `Laterality`) can take the value \"NC\" which stands for \"Not Communicated\". This label replaces missing data and depending on the variable may affect up to 2% of the database.\n",
    "\n",
    "- There are uncertainties in the definition of the starts and ends of the steps. Indeed, we can see on previous figures that the start and end could be slightly moved. However, our choice of metric is relatively immune to small variations in the start and end of footsteps.\n",
    "\n",
    "- There is a lot of variability in the step patterns depending on the pathology, the age, the weight, the sensor brand, etc. We invite the participants to skim through the different trials to see how footsteps vary. Generally, long signals (over 40 seconds) display pathological behaviours.\n",
    "\n",
    "- For a given trial, the two associated signals (left foot sensor and right foot sensor) have the same duration (and therefore the same shape). However they might not have the same number of annotated footsteps. Indeed, it often happens that one foot makes one step more compared to the other. Also, between trials, the number of signal samples greatly varies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# signal duration\n",
    "sampling_freq = 100  # Hz\n",
    "\n",
    "duration_list = list()\n",
    "for code in code_list:\n",
    "    sensor_data = load_human_locomotion_dataset(code)\n",
    "    duration_list += [sensor_data.signal.shape[0] / sampling_freq]\n",
    "\n",
    "print(\n",
    "    f\"On average, a recording lasts {get_avg_min_max(duration_list)} seconds;\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Compute the following:</p>\n",
    "    <ul>\n",
    "        <li>the average number of steps per trial,</li>\n",
    "        <li>the average foostep duration,</li>\n",
    "        <li>the number of trials for each pathology.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Number of steps per recording per foot\n",
    "n_steps_list = list()\n",
    "for code in code_list:\n",
    "    sensor_data = load_human_locomotion_dataset(code)\n",
    "    ...\n",
    "\n",
    "# Footstep duration\n",
    "...\n",
    "\n",
    "# Pathology distribution\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Which is the pathology group of the patient with the longuest trial?</p>\n",
    "    <p>Plot its signal.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the locogram for the trial at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locogram = get_locogram(sensor_data=sensor_data, left_or_right=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.heatmap(1 - locogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Compare the left and right locograms.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Compute the locograms for a trial with a few steps (between 10 and 15) and for a trial with many steps (more than 30).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric\n",
    "\n",
    "Step detection methods will be evaluated with the **F-score**, based on the following precision/recall definitions.\n",
    "The F-score is first computed per signal then averaged over all instances.\n",
    "\n",
    "Precision and recall rely on the \"intersection over union\" metric ($\\text{IoU}$) that measures the overlap of two intervals $[s_1,e_1]$ and $[s_2, e_2]$:\n",
    "\n",
    "$$\n",
    "\\text{IoU}=\\frac{\\big|[s_1,e_1]\\cap [s_2, e_2]\\big|}{\\big|[s_1,e_1]\\cup [s_2, e_2]\\big|}\n",
    "$$\n",
    "\n",
    "- Precision (or positive predictive value). A detected (or predicted) step is counted as correct if it overlaps (measured by $\\text{IoU}$) an annotated step by more than 75%. The precision is the number of correctly predicted steps divided by the total number of predicted steps.\n",
    "\n",
    "- Recall (or sensitivity). An annotated step is counted as detected if it overlaps (measured by $\\text{IoU}$) a predicted step by more than 75%. The recall is the number of detected annotated steps divided by the total number of annotated steps.\n",
    "\n",
    "\n",
    "The F-score is the geometric mean of the precision and recall: $$2\\times\\frac{\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}}.$$\n",
    "\n",
    "Note that an annotated step can only be detected once, and a predicted step can only be used to detect one annotated step.\n",
    "If several predicted steps correspond to the same annotated step, all but one are considered as false.\n",
    "Conversely, if several annotated steps are detected with the same predicted step, all but one are considered undetected.\n",
    "\n",
    "**Example 1.**\n",
    "\n",
    "- Annotation (\"ground truth label\"): $\\big[[80, 100], [150, 250], [260, 290]\\big]$ (three steps)\n",
    "- Prediction: $\\big[[80, 98], [105, 120], [256, 295], [298, 310]\\big]$ (four steps)\n",
    "\n",
    "Here, precision is $0.5=(1+0+1+0)/4$, recall is $0.67=(1+0+1)/3$ and the F-score is $0.57$.\n",
    "\n",
    "**Example 2.**\n",
    "\n",
    "- Annotation (\"ground truth label\"): $\\big[[80, 120]\\big]$ (one step)\n",
    "- Prediction: $\\big[[80, 95]\\big]$ (one step)\n",
    "\n",
    "Here, precision is $0=0/1$, recall is $0=0/1$ and the F-score is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single atom\n",
    "\n",
    "To illustrate, a simple step detection method is described now.\n",
    "- A random step is chosen (the template).\n",
    "- For the same signal, we perform a convolution sparse coding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a single dimension\n",
    "signal = sensor_data.signal.LRY.to_numpy()\n",
    "signal -= signal.mean()\n",
    "signal /= signal.std()\n",
    "\n",
    "n_samples = signal.shape[0]\n",
    "\n",
    "# take an arbitrary footstep\n",
    "start, end = sensor_data.left_steps[5]\n",
    "template = signal[start:end]\n",
    "template -= template.mean()\n",
    "template /= template.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the convolutional sparse coding into a regular sparse coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = get_dictionary_from_single_atom(\n",
    "    atom=template, n_samples=n_samples\n",
    ")\n",
    "sparse_codes = get_sparse_codes(\n",
    "    signal=signal, dictionary=dictionary, penalty=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDL(\n",
    "    signal=signal,\n",
    "    codes=sparse_codes,\n",
    "    atoms=template.reshape(1, -1),\n",
    "    figsize=(15, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>What is the influence of the penalty on the number of non-zero activations?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = fig_ax()\n",
    "reconstruction = sparse_codes.dot(dictionary).flatten()\n",
    "ax.plot(signal)\n",
    "ax.plot(reconstruction)\n",
    "ax.set_xlim(0, n_samples)\n",
    "_ = ax.set_title(f\"MSE = {((signal-reconstruction)**2).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>What is the influence of the penalty on the reconstruction error?</p>\n",
    "    <p>Is MSE a good measure to choose the penalty?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>How many steps were detected?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the detected steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_arr = argrelmax(sparse_codes.flatten(), order=10)[0]\n",
    "end_arr = start_arr + template.shape[0]\n",
    "detected_steps = np.c_[start_arr, end_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_0, ax_1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 3))\n",
    "\n",
    "# Color the footsteps\n",
    "ax = ax_0\n",
    "sensor_data.signal[[\"LAX\", \"LAY\", \"LAZ\", \"LAV\"]].plot(ax=ax)\n",
    "line_args = {\"linestyle\": \"--\", \"color\": \"k\"}\n",
    "for (start, end) in detected_steps:\n",
    "    ax.axvline(start, **line_args)\n",
    "    ax.axvline(end, **line_args)\n",
    "    ax.axvspan(start, end, facecolor=\"g\", alpha=0.3)\n",
    "\n",
    "ax = ax_1\n",
    "sensor_data.signal[[\"LRX\", \"LRY\", \"LRZ\", \"LRV\"]].plot(ax=ax)\n",
    "for (start, end) in detected_steps:\n",
    "    ax.axvline(start, **line_args)\n",
    "    ax.axvline(end, **line_args)\n",
    "    ax.axvspan(start, end, facecolor=\"g\", alpha=0.3)\n",
    "\n",
    "print(f\"F-score: {fscore(sensor_data.left_steps, detected_steps):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Using the `fscore`, choose an optimal value for the regularization parameter.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Using the same template, detect steps in a whole new signal (try with the neurological group).</p>\n",
    "    <p>What do you observe? What is your recommendation?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = code_list[1000]\n",
    "# loading a trial\n",
    "sensor_data = load_human_locomotion_dataset(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data.metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td-mva",
   "language": "python",
   "name": "td-mva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
